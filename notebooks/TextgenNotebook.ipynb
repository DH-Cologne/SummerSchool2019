{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation with Neural Networks\n",
    "\n",
    "Import necessary packages for preprocessing, model building, etc. We follow the steps described in the theoretical part of this summer school as follows:\n",
    "\n",
    "0. Define Reseach Goal (already done)\n",
    "2. Retrieve Data\n",
    "3. Prepare Data\n",
    "4. Explore Data\n",
    "5. Model Data\n",
    "6. Present and automate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Retrieve Data\n",
    "\n",
    "Load your data! You can pick up data from everywhere, such as plain text, HTML, source code, etc.\n",
    "You can either automatically download with Keras get_file function or download it manually and import it in this notebook.\n",
    "\n",
    "## Example Data Set\n",
    "[trump.txt](https://raw.githubusercontent.com/harshilkamdar/trump-tweets/master/trump.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = get_file('trump.txt', origin='https://raw.githubusercontent.com/harshilkamdar/trump-tweets/master/trump.txt')\n",
    "text = io.open('resources/shakespeare.txt', encoding='utf-8').read().lower()\n",
    "\n",
    "print('corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Prepare Data\n",
    "\n",
    "As described in the theoretical part of this workshop we need to convert our text into a word embedding that can be processed by a (later) defined Neural Network. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Create Classes \n",
    "The goal after this step is to have a variable which contains the distinct characters of the text. Characters can be letters, digits, punctions, new lines, spaces, etc.\n",
    "\n",
    "### Example:\n",
    "Let's assume we have the following text as input: \"hallo. \"\n",
    "\n",
    "After the following step, we want to have all distinct characters, i.e.:\n",
    "\n",
    "``[ \"h\", \"a\", \"l\", \"o\", \".\", \" \" ] ``\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Create Training Set\n",
    "\n",
    "In the following section we need to create our test set based on our text. The idea is to map a sequence of characters to a class. In this case, a class is one of our distinct characters defined in the previous task. This means that a sequence of characters predicts the next character. This is important for the later model to know which characters come after specific sequences. The sequence length can be chosen. So try out different squence length.\n",
    "\n",
    "### Example:\n",
    "Our text is still: \"hallo. \"\n",
    "Sequence length: 2 (i.e. 2 characters predict the next character)\n",
    "\n",
    "The result (training set) should be defined as follows:\n",
    "\n",
    "``\n",
    "Seuences --> Class\n",
    " \"ha\"    --> \"l\"\n",
    " \"al\"    --> \"l\"\n",
    " \"ll\"    --> \"o\"\n",
    " \"lo\"    --> \".\"\n",
    " \"o.\"    --> \" \"\n",
    "``\n",
    "\n",
    "You can read the previous example like this: Squence \"ha\" predicts the next character \" l \", sequence \"al\" predicts next character \" l \" and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqlen = 40 # Sequence length parameter\n",
    "step = 5   # Determines the how many characters the window should be shifted in the text \n",
    "sequences = []  # List of sequences\n",
    "char_class = [] # Corresponding class of each sequence\n",
    "\n",
    "for i in range(0, len(text) - seqlen, step):\n",
    "    sequences.append(text[i: i + seqlen])\n",
    "    char_class.append(text[i + seqlen])\n",
    "print('#no sequences:', len(sequences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Check your Data\n",
    "\n",
    "Now that we processed our data, it's time to understand what we have built so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(sequences[:10])):\n",
    "    print(sequences[idx], \":\" , char_class[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print from 1st to 10th character \n",
    "chars[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print from 150th to 160th character :-)\n",
    "chars[150:160]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Vectorization of Training Sequences\n",
    "\n",
    "The following section describes the desired form of our final training set. \n",
    "\n",
    "text: \"hallo. \".\n",
    "As defined above we have a couple of sequences mapping to the next appearing character in the text (e.g. \"ha\" mapping to \"l\"). But first of all, we transform each sequence to the following one-hot-encoded matrix.\n",
    "\n",
    "**Example:** \n",
    "sequence \"ha\" maps to the following matrix\n",
    "\n",
    "|     |  h  |  a  |  l  |  o  |  .  | ' ' |\n",
    "|-----|-----|-----|-----|-----|-----|-----|\n",
    "|  h  |  1  |  0  |  0  |  0  |  0  |  0  |\n",
    "|  a  |  0  |  1  |  0  |  0  |  0  |  0  |\n",
    "\n",
    "next sequence \"al\" maps to the following matrix\n",
    "\n",
    "|     |  h  |  a  |  l  |  o  |  .  | ' ' |\n",
    "|-----|-----|-----|-----|-----|-----|-----|\n",
    "|  a  |  0  |  1  |  0  |  0  |  0  |  0  |\n",
    "|  l  |  0  |  0  |  1  |  0  |  0  |  0  |\n",
    "\n",
    "... And so on\n",
    "\n",
    "## 2.5. Vectorization of Target Classes\n",
    "\n",
    "We build our target classes similar to the training set. We need a one hot-encoded vector for each target (which is a character).\n",
    "\n",
    "**Example:** for target char \"l\" the vector looks like this\n",
    "\n",
    "|     |  h  |  a  |  l  |  o  |  .  | ' ' |\n",
    "|-----|-----|-----|-----|-----|-----|-----|\n",
    "|  l  |  0  |  0  |  1  |  0  |  0  |  0  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexed characters as dictionary\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "\n",
    "# Both matrices will initialized with zeros\n",
    "training_set = np.zeros((len(sequences), seqlen, len(chars)), dtype=np.bool)\n",
    "target_char = np.zeros((len(sequences), len(chars)), dtype=np.bool)\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, char in enumerate(sequence):\n",
    "        training_set[i, t, char_indices[char]] = 1\n",
    "    target_char[i, char_indices[char_class[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the shape of the training_set\n",
    "\n",
    "training_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output: (x, y, z)\n",
    "\n",
    "    x = number of all sequences to test\n",
    "    y = window size to predict the next character\n",
    "    z = number of all appearing characters in text (for one-hot-enconding) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the shape of the target_char (act as our target classes)\n",
    "\n",
    "target_char.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output: (x, y)\n",
    "\n",
    "    x = number of all sequences to test\n",
    "    y = the mapping of each sequence to the next character\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model data\n",
    "\n",
    "Let's get down to business! Create your model.\n",
    "\n",
    "Try different model configuration (see [keras doc](https://keras.io/models/about-keras-models/#about-keras-models)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# build the model: a LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(seqlen, len(chars))))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNextCharIdx(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of reverse char index, to get the char for the predicted class\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "def on_epoch_end(epoch, logs):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "    start_index = random.randint(0, len(text) - seqlen - 1)\n",
    "    for diversity in [1, 0.1, 0.5]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + seqlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(1000):\n",
    "            x_pred = np.zeros((1, seqlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = getNextCharIdx(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluate Model\n",
    "\n",
    "We are not at the sweet part of the model. Let's fit our model and see what it prints!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(training_set, target_char,\n",
    "          batch_size=128,\n",
    "          epochs=3,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Present and Automate\n",
    "\n",
    "Having a model trained for hours is a valuable asset! We need now to store the model and use it to solve the problem we wanted to solve with Machine Learning. Keras has a simple function to save a model to the local file system and also a function to load the model again and have it ready for our task!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('shakespeareModel.h5')\n",
    "model = load_model('shakespeareModel.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
