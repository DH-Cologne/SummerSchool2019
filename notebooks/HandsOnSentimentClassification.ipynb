{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Define Research Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to train a model that's able to classify movie reviews into a positive and a negative class.\n",
    "\n",
    "First, we need to import all python libraries that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.models import Sequential\n",
    "from keras import metrics\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "import sklearn.datasets as skds\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Retrieve Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IMDb Reviews are separated into training and test folders. Each of them has folders for positive and negative reviews.\n",
    "\n",
    "First, we import all training data to create and train our neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1237)\n",
    "\n",
    "labels = [\"pos\", \"neg\"] # contains all category labels that we want to classify\n",
    "num_labels = 2 # number of labels\n",
    "\n",
    "path_train = \"./resources/aclImdb/train\" # path to all reviews that we want to use for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data by using [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_files.html). Afterwards, files_train should contain the path and label for each training review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_train = skds.load_files(path_train,load_content=False, categories=labels, encoding=\"UTF-8\") \n",
    "\n",
    "file_paths = files_train.filenames\n",
    "label_names = files_train.target_names\n",
    "labelled_files_index = files_train.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we start to read all reviews (This might take some time). Afterwards, data_list should contain tuples of file path, file label and file content for each review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "\n",
    "for i, file in enumerate(file_paths):\n",
    "    data_list.append((file,\n",
    "                      label_names[labelled_files_index[i]],\n",
    "                      Path(file).read_text(encoding=\"UTF-8\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuples are transformed into [pandas DataFrame](https://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.DataFrame.from_records.html). data.head() should show your DataFrame like this:\n",
    "<img src=\"resources/dataframe.png\" alt=\"Data Frame Example\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tags=[\"filename\",\"category\",\"review\"]\n",
    "data = pd.DataFrame.from_records(data_list, columns=data_tags)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a Data Frame with all training reviews. For developing the Neural Network we split the DataFrame into training (80%) and development (20%) set. For training, we take review, category and file name from the first 80% of the Data Frame entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(data) * .8) # number of reviews that we take for training\n",
    "\n",
    "train_reviews = data['review'][:train_size]\n",
    "train_tags = data['category'][:train_size]\n",
    "train_files_names = data['filename'][:train_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make the reviews interpretable for the Neural Network, we need to tokenize and vectorize the content of the reviews. We use the Keras [Tokenizer](http://faroit.com/keras-docs/1.2.2/preprocessing/text/#tokenizer) to split each review into tokens.\n",
    "\n",
    "The tokens of each review are weighted corresponding to the selected mode. \"Binary\" sets a 1 for a token if it appears in the review and 0 if it doesn't appear.\n",
    "\n",
    "**Now, it's your turn**: Change 'vocab_size' and 'mode' and see what happens to the neural net!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5000 # determines the size of the vocabulary\n",
    "mode = 'binary'\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(train_reviews)\n",
    "\n",
    "x_train = tokenizer.texts_to_matrix(train_reviews, mode=mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see how our train matrix looks like (prints out the first 10 review vectors):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save the tokenizer to a pickle file (for example to load it in the evaluation notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('resources/tokenizer/defaulttokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Label Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to encode the categories of our reviews, too. We can do it with a LabelBinarizer that produces an array of all tagged categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelBinarizer()\n",
    "encoder.fit(train_tags)\n",
    "y_train = encoder.transform(train_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Explore Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get to know the data a little bit better you can count how many positive and negative reviews are there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val, count = np.unique(y_train, return_counts=True) #count frequency of each number in y_train\n",
    "\n",
    "for i, c in enumerate(count):\n",
    "    label = encoder.inverse_transform(val[i])\n",
    "    print(label, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have a matrix of our train input and an array of the related categories. We can build a neural net and train it with x_train and y_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = 'adam'\n",
    "loss = 'binary_crossentropy'\n",
    "batch_size = 10\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()\n",
    "#First Hidden Layer\n",
    "classifier.add(Dense(4, activation='relu', kernel_initializer='random_normal', input_dim=vocab_size))\n",
    "#Second  Hidden Layer\n",
    "classifier.add(Dense(4, activation='relu', kernel_initializer='random_normal'))\n",
    "#Output Layer\n",
    "classifier.add(Dense(1, activation='sigmoid', kernel_initializer='random_normal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compiling the neural network\n",
    "classifier.compile(optimizer=optimizer,loss=loss, \n",
    "                   metrics =['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the data to the training dataset\n",
    "classifier.fit(x_train,y_train, batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save('resources/models/defaultModel.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify new Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"classifier\" contains our trained neural net. We can use it to classify new film reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "own_review = \"Greatest film ever\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_series = pd.Series(own_review)\n",
    "x_review = tokenizer.texts_to_matrix(review_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = classifier.predict(x_review)\n",
    "prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on Eval Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_reviews = data['review'][train_size:]\n",
    "eval_tags = data['category'][train_size:]\n",
    "eval_files_names = data['filename'][train_size:]\n",
    "\n",
    "x_eval = tokenizer.texts_to_matrix(eval_reviews, mode=mode)\n",
    "y_eval = encoder.transform(eval_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = classifier.predict(x_eval)\n",
    "y_classified = (probs>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_eval, y_classified)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = encoder.classes_\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm)\n",
    "fig.colorbar(cax)\n",
    "ax.set_yticks(np.arange(len(labels)))\n",
    "ax.set_xticks(np.arange(len(labels)))\n",
    "ax.set_xticklabels(labels, rotation='vertical')\n",
    "ax.set_yticklabels(labels)\n",
    "#ax.set_yticklabels(np.arrage(len(test_tags)), test_tags)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until here, you can try to improve your neural net by using the training data. If you think your configurations are ready, you can evaluate them by using the **EvaluateSentimentClassification** Notebook.\n",
    "\n",
    "*** "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
