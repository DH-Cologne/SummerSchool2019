{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Define Research Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to train a model that's able to classify movie reviews into a positive and a negative class.\n",
    "\n",
    "First, we need to import all python libraries that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.models import Sequential\n",
    "from keras import metrics\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "import sklearn.datasets as skds\n",
    "from sklearn import metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Retrieve Data\n",
    "The IMDb Reviews are separated into training and test folders. Each of them has folders for positive and negative reviews.\n",
    "\n",
    "First, we import all training data to create and train our neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1237)\n",
    "\n",
    "labels = [\"pos\", \"neg\"] # contains all category labels that we want to classify\n",
    "num_labels = 2 # number of labels\n",
    "\n",
    "path_train = \"./resources/aclImdb/train\" # path to all reviews that we want to use for developing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data by using [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_files.html). Afterwards, 'files_train' should contain the path and label for each training review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_train = skds.load_files(path_train,load_content=False, categories=labels, encoding=\"UTF-8\") \n",
    "\n",
    "file_paths = files_train.filenames\n",
    "label_names = files_train.target_names\n",
    "labelled_files_index = files_train.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we start to read all reviews. Afterwards, data_list should contain tuples of file path, file label and file content for each review.\n",
    "\n",
    "If this step takes too long, interrupt it and skip to \"Load data from pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "\n",
    "for i, file in enumerate(file_paths):\n",
    "    data_list.append((file,\n",
    "                      label_names[labelled_files_index[i]],\n",
    "                      Path(file).read_text(encoding=\"UTF-8\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuples are transformed into [pandas DataFrame](https://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.DataFrame.from_records.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tags=[\"filename\",\"category\",\"review\"]\n",
    "data = pd.DataFrame.from_records(data_list, columns=data_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Load data from pickle\n",
    "If reading the data from folders takes too long, you can load the data frame from a serialized pickle file. (Skip this step, if your data frame is already created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('resources/train_dataframe.pickle', 'rb') as handle:\n",
    "    data = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data.head() should show your DataFrame like this:\n",
    "<img src=\"resources/dataframe.png\" alt=\"Data Frame Example\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>category</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./resources/aclImdb/train/pos/11485_10.txt</td>\n",
       "      <td>pos</td>\n",
       "      <td>Zero Day leads you to think, even re-think why...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./resources/aclImdb/train/neg/6802_1.txt</td>\n",
       "      <td>neg</td>\n",
       "      <td>Words can't describe how bad this movie is. I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./resources/aclImdb/train/pos/7641_10.txt</td>\n",
       "      <td>pos</td>\n",
       "      <td>Everyone plays their part pretty well in this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./resources/aclImdb/train/neg/9698_1.txt</td>\n",
       "      <td>neg</td>\n",
       "      <td>There are a lot of highly talented filmmakers/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./resources/aclImdb/train/neg/3141_2.txt</td>\n",
       "      <td>neg</td>\n",
       "      <td>I've just had the evidence that confirmed my s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     filename category  \\\n",
       "0  ./resources/aclImdb/train/pos/11485_10.txt      pos   \n",
       "1    ./resources/aclImdb/train/neg/6802_1.txt      neg   \n",
       "2   ./resources/aclImdb/train/pos/7641_10.txt      pos   \n",
       "3    ./resources/aclImdb/train/neg/9698_1.txt      neg   \n",
       "4    ./resources/aclImdb/train/neg/3141_2.txt      neg   \n",
       "\n",
       "                                              review  \n",
       "0  Zero Day leads you to think, even re-think why...  \n",
       "1  Words can't describe how bad this movie is. I ...  \n",
       "2  Everyone plays their part pretty well in this ...  \n",
       "3  There are a lot of highly talented filmmakers/...  \n",
       "4  I've just had the evidence that confirmed my s...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Prepare Data\n",
    "We now have a Data Frame with all training reviews. For developing the Neural Network we split the DataFrame into training (80%) and development (20%) set. For training, we take review, category and file name from the first 80% of the Data Frame entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(data) * .8) # number of reviews that we take for training\n",
    "\n",
    "train_reviews = data['review'][:train_size]\n",
    "train_tags = data['category'][:train_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Vectorization\n",
    "In order to make the reviews interpretable for the Neural Network, we need to tokenize and vectorize the content of the reviews. We use the Keras [Tokenizer](http://faroit.com/keras-docs/1.2.2/preprocessing/text/#tokenizer) to split each review into tokens.\n",
    "\n",
    "The tokens of each review are weighted corresponding to the selected mode. \"Binary\" sets a 1 for a token if it appears in the review and 0 if it doesn't appear.\n",
    "\n",
    "**Now, it's your turn**: Change 'vocab_size' and 'mode' and see what happens to the neural net!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5000 # determines the size of the vocabulary\n",
    "mode = 'binary'\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(train_reviews)\n",
    "\n",
    "x_train = tokenizer.texts_to_matrix(train_reviews, mode=mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see how our train matrix looks like (prints out the first 10 review vectors):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 1., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save the tokenizer to a pickle file (for example to load it in the evaluation notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('resources/tokenizer/defaulttokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Label Encoding\n",
    "We need to encode the categories of our reviews, too. We can do it with a LabelBinarizer that produces an array of all tagged categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelBinarizer()\n",
    "encoder.fit(train_tags)\n",
    "y_train = encoder.transform(train_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Explore Data\n",
    "To get to know the data a little bit better you can count how many positive and negative reviews are there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg'] 10008\n",
      "['pos'] 9992\n"
     ]
    }
   ],
   "source": [
    "val, count = np.unique(y_train, return_counts=True) #count frequency of each number in y_train\n",
    "\n",
    "for i, c in enumerate(count):\n",
    "    label = encoder.inverse_transform(val[i])\n",
    "    print(label, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Data\n",
    "Now, we have a matrix of our train input and an array of the related categories. We can build a neural net and train it with x_train and y_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = 'adam'\n",
    "loss = 'binary_crossentropy'\n",
    "batch_size = 10\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "classifier = Sequential()\n",
    "#First Hidden Layer\n",
    "classifier.add(Dense(4, activation='relu', kernel_initializer='random_normal', input_dim=vocab_size))\n",
    "#Second  Hidden Layer\n",
    "classifier.add(Dense(4, activation='relu', kernel_initializer='random_normal'))\n",
    "#Output Layer\n",
    "classifier.add(Dense(1, activation='sigmoid', kernel_initializer='random_normal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compiling the neural network\n",
    "classifier.compile(optimizer=optimizer,loss=loss, \n",
    "                   metrics =['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 2s 102us/step - loss: 0.3711 - acc: 0.8387\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 2s 90us/step - loss: 0.2250 - acc: 0.9139\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 2s 90us/step - loss: 0.1924 - acc: 0.9273\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 2s 90us/step - loss: 0.1745 - acc: 0.9335\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 2s 90us/step - loss: 0.1590 - acc: 0.9395\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 2s 90us/step - loss: 0.1480 - acc: 0.9453\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 2s 90us/step - loss: 0.1393 - acc: 0.9490\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 2s 90us/step - loss: 0.1308 - acc: 0.9524\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 2s 89us/step - loss: 0.1207 - acc: 0.9570\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 2s 90us/step - loss: 0.1126 - acc: 0.9605\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2374d9c9b0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting the data to the training dataset\n",
    "classifier.fit(x_train,y_train, batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save('resources/models/defaultModel.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify new Reviews\n",
    "\"classifier\" contains our trained neural net. We can use it to classify new film reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "own_review = \"Greatest film ever\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_series = pd.Series(own_review)\n",
    "x_review = tokenizer.texts_to_matrix(review_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7411214]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob = classifier.predict(x_review)\n",
    "prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Improve Model\n",
    "After trying the neural net on example reviews we want to know how good it works in general. We use the validation set to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_reviews = data['review'][train_size:]\n",
    "val_tags = data['category'][train_size:]\n",
    "\n",
    "x_val = tokenizer.texts_to_matrix(val_reviews, mode=mode)\n",
    "y_val = encoder.transform(val_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we [predict](https://keras.io/models/model/#predict) labels for all test reviews. If the probability for a positive review is more than 0.5, \"pos\" will be assigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pos', 'pos', 'neg', 'pos', 'pos', 'pos', 'pos', 'pos', 'neg', 'pos']\n",
      "['pos', 'pos', 'neg', 'neg', 'neg', 'pos', 'pos', 'pos', 'pos', 'neg']\n"
     ]
    }
   ],
   "source": [
    "probs = classifier.predict(x_val)\n",
    "y_classified = ['pos' if x > 0.5 else 'neg' for x in probs]\n",
    "\n",
    "y_true = list(encoder.inverse_transform(y_val)) #transform true encoded categories (0 and 1) to labels (neg and pos)\n",
    "\n",
    "print(y_classified[:10]) #print first 10 predictions and true labels\n",
    "print(y_true[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a [confusion matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) to compare all predicted labels(y_classified) with the true labels(y_true)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2127,  365],\n",
       "       [ 330, 2178]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = metrics.confusion_matrix(y_true, y_classified, labels=[\"neg\", \"pos\"])\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix gives us all values for further evaluation computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN, FP, FN, TP  (2127, 365, 330, 2178)\n",
      "Precision  0.8564687377113646\n",
      "Recall  0.868421052631579\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = cm.ravel()\n",
    "pre = metrics.precision_score(y_true, y_classified, pos_label='pos')\n",
    "rec = metrics.recall_score(y_true, y_classified, pos_label='pos')\n",
    "print(\"TN, FP, FN, TP \", (tn, fp, fn, tp))\n",
    "print(\"Precision \", pre)\n",
    "print(\"Recall \", rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can improve your model by adjusting the configurations. If you think your configurations are ready, you can evaluate them with the test data by using the **EvaluateSentimentClassification** Notebook.\n",
    "\n",
    "*** "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
