{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Define Research Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to train a model that's able to classify movie reviews into a positive and a negative class.\n",
    "\n",
    "First, we need to import all python libraries that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.models import Sequential\n",
    "from keras import metrics\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "import sklearn.datasets as skds\n",
    "from sklearn import metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Retrieve Data\n",
    "The IMDb Reviews are separated into training and test folders. Each of them has folders for positive and negative reviews.\n",
    "\n",
    "First, we import all training data to create and train our neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1237)\n",
    "\n",
    "labels = [\"pos\", \"neg\"] # contains all category labels that we want to classify\n",
    "num_labels = 2 # number of labels\n",
    "\n",
    "path_train = \"./resources/aclImdb/train\" # path to all reviews that we want to use for developing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data by using [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_files.html). Afterwards, 'files_train' should contain the path and label for each training review.\n",
    "\n",
    "Then, we start to read all reviews. Afterwards, data_list should contain tuples of file path, file label and file content for each review.\n",
    "\n",
    "If this step takes too long, interrupt it and skip to \"Load data from pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_train = skds.load_files(path_train,load_content=False, categories=labels, encoding=\"UTF-8\") \n",
    "\n",
    "file_paths = files_train.filenames\n",
    "label_names = files_train.target_names\n",
    "labelled_files_index = files_train.target\n",
    "\n",
    "data_list = []\n",
    "for i, file in enumerate(file_paths):\n",
    "    data_list.append((file,\n",
    "                      label_names[labelled_files_index[i]],\n",
    "                      Path(file).read_text(encoding=\"UTF-8\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuples are transformed into [pandas DataFrame](https://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.DataFrame.from_records.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tags=[\"filename\",\"category\",\"review\"]\n",
    "data = pd.DataFrame.from_records(data_list, columns=data_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Load data from pickle\n",
    "If reading the data from folders takes too long, you can load the data frame from a serialized pickle file. (Skip this step, if your data frame is already created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('resources/dataframes/train_dataframe.pickle', 'rb') as handle:\n",
    "    data = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data.head() should show your DataFrame like this:\n",
    "<img src=\"resources/dataframe.png\" alt=\"Data Frame Example\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Prepare Data\n",
    "We now have a Data Frame with all training reviews. For developing the Neural Network we split the DataFrame into training (80%) and development (20%) set. For training, we take review, category and file name from the first 80% of the Data Frame entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(data) * .8) # number of reviews that we take for training\n",
    "\n",
    "train_reviews = data['review'][:train_size]\n",
    "train_tags = data['category'][:train_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Vectorization\n",
    "In order to make the reviews interpretable for the Neural Network, we need to tokenize and vectorize the content of the reviews. We use the Keras [Tokenizer](http://faroit.com/keras-docs/1.2.2/preprocessing/text/#tokenizer) to split each review into tokens.\n",
    "\n",
    "The tokens of each review are weighted corresponding to the selected mode. \"Binary\" sets a 1 for a token if it appears in the review and 0 if it doesn't appear.\n",
    "\n",
    "**Now, it's your turn**: Change 'vocab_size' and 'mode' and see what happens to the neural net!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5000 # determines the size of the vocabulary\n",
    "mode = 'binary'\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(train_reviews)\n",
    "\n",
    "x_train = tokenizer.texts_to_matrix(train_reviews, mode=mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see how our train matrix looks like (prints out the first 10 review vectors):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'resources/models/newmodel/' # adjust this path if you want to save a new model\n",
    "if not os.path.isdir(model_path):\n",
    "    os.mkdir(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save the tokenizer to a pickle file (for example to load it in the evaluation notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_path +'defaulttokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Label Encoding\n",
    "We need to encode the categories of our reviews, too. We can do it with a LabelBinarizer that produces an array of all tagged categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelBinarizer()\n",
    "encoder.fit(train_tags)\n",
    "y_train = encoder.transform(train_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Explore Data\n",
    "To get to know the data a little bit better you can count how many positive and negative reviews are there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val, count = np.unique(y_train, return_counts=True) #count frequency of each encoded label in y_train\n",
    "\n",
    "for i, c in enumerate(count):\n",
    "    label = encoder.inverse_transform(val[i])\n",
    "    print(label, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can check the dimensions of our training data.<br>\n",
    "Output: (x = number of training reviews, y = size of each vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can explore the vocabulary of our tokenizer. 'vocab' is sorted decreasing by the document frequency of the tokens. 'sum_words' is a vector that contains the document frequency of each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokenizer.word_index\n",
    "sum_words = x_train.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can explore the document frequencies by asking for specific tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_actress = vocab['actress']\n",
    "index_actor = vocab['actor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum_words[index_actress])\n",
    "print(sum_words[index_actor])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Data\n",
    "Now, we have a matrix of our train input and an array of the related categories. We can build a neural net and train it with x_train and y_train.\n",
    "\n",
    "**Now, it's your turn:** Change Parameters like optimizer, layer number, layer activation, layer size etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = 'adam'\n",
    "loss = 'binary_crossentropy'\n",
    "batch_size = 10\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()\n",
    "#First Hidden Layer\n",
    "classifier.add(Dense(4, activation='relu', kernel_initializer='random_normal', input_dim=vocab_size))\n",
    "#Second Hidden Layer\n",
    "classifier.add(Dense(4, activation='relu', kernel_initializer='random_normal'))\n",
    "#Output Layer\n",
    "classifier.add(Dense(1, activation='sigmoid', kernel_initializer='random_normal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compiling the neural network\n",
    "classifier.compile(optimizer=optimizer,loss=loss, \n",
    "                   metrics =['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the data to the training dataset\n",
    "classifier.fit(x_train,y_train, batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save(model_path +'neuralnet.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify new Reviews\n",
    "\"classifier\" contains our trained neural net. We can use it to classify new film reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "own_review = \"Bad bad film!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_series = pd.Series(own_review)\n",
    "x_review = tokenizer.texts_to_matrix(review_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = classifier.predict(x_review)\n",
    "prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Improve Model\n",
    "After trying the neural net on example reviews we want to know how good it works in general. We use the validation set to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_reviews = data['review'][train_size:]\n",
    "val_tags = data['category'][train_size:]\n",
    "\n",
    "x_val = tokenizer.texts_to_matrix(val_reviews, mode=mode)\n",
    "y_val = encoder.transform(val_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we [predict](https://keras.io/models/model/#predict) labels for all test reviews. If the probability for a positive review is more than 0.5, \"pos\" will be assigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = classifier.predict(x_val)\n",
    "y_classified = ['pos' if x > 0.5 else 'neg' for x in probs]\n",
    "\n",
    "y_true = list(encoder.inverse_transform(y_val)) #transform true encoded categories (0 and 1) to labels (neg and pos)\n",
    "\n",
    "print(y_classified[:10]) #print first 10 predictions and true labels\n",
    "print(y_true[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a [confusion matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) to compare all predicted labels(y_classified) with the true labels(y_true)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(y_true, y_classified, labels=[\"neg\", \"pos\"])\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix gives us all values for further evaluation computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = cm.ravel()\n",
    "pre = metrics.precision_score(y_true, y_classified, pos_label='pos')\n",
    "rec = metrics.recall_score(y_true, y_classified, pos_label='pos')\n",
    "print(\"TN, FP, FN, TP \", (tn, fp, fn, tp))\n",
    "print(\"Precision \", pre)\n",
    "print(\"Recall \", rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can improve your model by adjusting the configurations. If you think your configurations are ready, you can evaluate them with the test data by using the **EvaluateSentimentClassification** Notebook.\n",
    "\n",
    "*** "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
